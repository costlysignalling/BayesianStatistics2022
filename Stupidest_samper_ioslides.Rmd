---
title: "Stupidest sampler presentation"
author: "Petr Tureƒçek"
date: "8 1 2023"
output: ioslides_presentation
---

```{r setup, echo=FALSE, warning=F, error=F, message=F}
library(rethinking)
set_ulam_cmdstan(FALSE)
set.seed(42)
```

## Before presenting
Hit Ctrl + Plus a couple of times, then F (full screen). Depending on the resolution of the projector, I may also need to press W.

# Bayesian statistics
## 01 The supidest sampler ever

## We have some coin tosses

```{r, echo=T}
data<-c("heads","tails","heads","heads")
```

## Look at them

```{r}
data
```


## How do we even begin to think about this?
The most straightforward way probably is to simulate both options, generate 4 coin tosses with each and see, how often each coin generates "heads","tails","heads","heads".

### This is a fair coin
```{r}
sample(c("heads","tails"),size=4,prob=c(0.5,0.5),replace=T)
```

### This is an unfair (trick) coin
```{r}
sample(c("heads","tails"),size=4,prob=c(0.8,0.2),replace=T)
```

## One toss is not enough.
I will modify the code to do four coin tosses with each coin 1000 times.

```{r}
d.fair<-replicate(n=1000,sample(c("heads","tails"),size=4,prob=c(0.5,0.5),replace=T))
str(d.fair)
```

## I do not like this in "wide" format
with few rows and many columns (transpose)

```{r}
d.fair<-t(d.fair)
head(d.fair)
```

This is more like it!

## The equivalent for an unfair coin.
Notice that I put the output directly into the transpose function

```{r}
d.tric<-t(replicate(n=1000,sample(c("heads","tails"),size=4,prob=c(0.8,0.2),replace=T)))
head(d.tric)
```

## So ok.
How many cointoss sequences with each coin are exactly like mine

```{r}
pasted.data<-paste(data,collapse=" ")

pasted.fair<-apply(d.fair,1,paste,collapse=" ")
pasted.tric<-apply(d.tric,1,paste,collapse=" ")

str(pasted.fair)
str(pasted.tric)
```


## How many are identical in each simulated set?

```{r}
pasted.data==pasted.fair
```

In total (remember, TRUE is 1, FALSE is 0)
```{r}
sum(pasted.data==pasted.fair)
sum(pasted.data==pasted.tric)
```

## So what is the resulting probability?
From two hypotheses that I considered (that can be summarized as "heads falls with 50% probability" and "head falls with 80% probability"), I got

```{r}
match.fair<-sum(pasted.data==pasted.fair)
match.tric<-sum(pasted.data==pasted.tric)
match.fair+match.tric
```
cases altogether

## This many
```{r}
match.fair
```
of them were generated by the simulated fair coin

## This many
```{r}
match.tric
```
of them (slightly more) by the trick coin

## Learn from simulations
So my belief about how my original data were generated should change accordingly to these probabilities

```{r}
(simul.p1<-match.fair/(match.fair+match.tric))
```
is the probability that is was the fair coin

```{r}
(simul.p2<-match.tric/(match.fair+match.tric))
```

Divide so the probabilities of the considered hypotheses always add up to 1 (it must have been one of the considered coins).

## Going more abstract
We need to make our system simple and similar to other systems, so we treat each heads as TRUE (or 1) and each tails as FALSE (or 0)

```{r}
data<-data=="heads"
data
```

## We can now compute
not simulate - the probability of data such as ours with the two possible coins.

With the tric coin, the probability of TRUE (heads) is p=0.8, and the probability of FALSE (tails) is (1-p)=0.2.

Focus on the understanding of why probabilities of two events in the row are a result of multiplication of their probabilities. (Draw forking paths)

## Two cointosses

```{r, echo=F}
plot(NULL,xlim=c(-2,2),ylim=c(0,2),bty="n",axes=F,xlab="",ylab="")
points(0,0,cex=2,lwd=2,pch=21,bg=0)
segments(c(0,0),c(0,0),c(-1,1),c(1,1),lwd=c(4,1))
points(c(-1,1),c(1,1),cex=2,lwd=2,pch=21,bg=0)
segments(c(-1,-1),c(1,1),-1+c(-1,1)*0.8,1+c(1,1)*0.8,lwd=c(4,1))
segments(c(1,1),c(1,1),1+c(-1,1)*0.8,1+c(1,1)*0.8,lwd=c(4,1))
points(-1+c(-1,1)*0.8,1+c(1,1)*0.8,cex=2,lwd=2,pch=21,bg=0)
points(1+c(-1,1)*0.8,1+c(1,1)*0.8,cex=2,lwd=2,pch=21,bg=0)
text(c(-1+c(-1,1)*0.8,1+c(-1,1)*0.8),c(1+c(1,1)*0.8,1+c(1,1)*0.8),
     labels=c("0.8*0.8=0.64",
            "0.8*0.2=0.16",
            "0.2*0.8=0.16",
            "0.2*0.2=0.04"),pos=c(3,3,4,3))
```


## It is easy, right?
So the probability of **TRUE FALSE TRUE TRUE** with coin characterized by a parameter of heads probability **p=0.8** is

```{r}
(calcul.l2<-0.8*0.2*0.8*0.8)
```

And the probability of the same data with the fair coin is
```{r}
(calcul.l1<-0.5*0.5*0.5*0.5)
```

## L for likelihood
What I am calculating could have been written like this: 
$$\text{likelihood of data if hypothesis} = P(d|H)$$

## Divide by sum,
Because it must have been one of the two, there are no other options.

```{r}
calcul.p1<-calcul.l1/(calcul.l1+calcul.l2)
calcul.p2<-calcul.l2/(calcul.l1+calcul.l2)
```
```{r,echo=F}
calcul.p1
calcul.p2
```

Notice how similar it is to the procedure that gave me
```{r}
simul.p1 #and
simul.p2
```

## Always divide by the sum of all the equivalent alternatives.

The procedure that we demonstrate above is already very close to the very intuitive thing that is for some reason labeled "Bayes' theorem".

## Now we are at simple
"probability that a hypothesis is correct is proportional to the likelihood that given hypothesis generated data that we observe".

$$P(H|d) = \frac{P(d|H)}{P(d)}$$

## What is this $P(d)$?

You can read it as "Probability of data regardless of the hypothesis". How do we achieve this "regardless of hypothesis"? You have seen it twice already. You just sum up data likelihood over all the hypothesis that you even consider.

# Remember, in our worlds, we never observe hypotheses. We always observe only data.

## General models tend to be better
Before we bring it home, let us thing for a moment about the level of generality that we aim for in our analyses. There is no right or wrong here, but more general models are usually more useful and the statistics done on them is usually (when we scale to really big ones) way quicker.

## Answer this for yourself
Are we really concerned in what order did the heads or tails fell? Is the data relevant for us really "heads tails heads heads" or is it "three heads out of four"?


## If we just sum the data
We get the number of heads. We get to the level of abstraction on which we will from now on treat data like this.
```{r}
sum(data)
```

So we got to a more abstract level - our data now basically say "3 out of 4 came out TRUE"

## Best book of statistics is Wikipedia
It is perfect that such a simple scenario has a very straigtforward analytical properties.
<https://en.wikipedia.org/wiki/Binomial_distribution> 
Watch for PMF, whcih stands for Probability Mass Function.

## Combination number in R
```{r}
choose(5,2)
```

## 3 scenarios

```{r,eval=F}
choose(2,2) #2 heads out of two
choose(2,0) #0 heads, all tails
choose(2,1) #1 of each
```
```{r,echo=F}
choose(2,2) #2 heads out of two
choose(2,0) #0 heads, all tails
choose(2,1) #1 of each
```

We can get back to our plot that tossed two unfair coins in a row.

## Two unfair cointosses
```{r,echo=F}
plot(NULL,xlim=c(-2,2),ylim=c(0,2),bty="n",axes=F,xlab="",ylab="")
points(0,0,cex=2,lwd=2,pch=21,bg=0)
segments(c(0,0),c(0,0),c(-1,1),c(1,1),lwd=c(4,1))
points(c(-1,1),c(1,1),cex=2,lwd=2,pch=21,bg=0)
segments(c(-1,-1),c(1,1),-1+c(-1,1),1+c(1,1),lwd=c(4,1))
segments(c(1,1),c(1,1),1+c(-1,1),1+c(1,1),lwd=c(4,1))
points(c(-2,0,2),c(2,2,2),cex=2,lwd=2,pch=21,bg=0)
text(c(-2,0,2),c(2,2,2),
     labels=c("1*0.8*0.8=0.64",
              "2*0.8*0.2=0.32",
              "1*0.2*0.2=0.04"),pos=c(3,3,3),xpd=NA)
```


## Calculate probabilities
Since probability of **TRUE FALSE** is $0.8*0.2=0.16$ in our example and the probability of **FLASE TRUE** is also $0.2*0.8=0.16$, the probability of the one step more abstract construct "one head out of two tosses" is $2*0.2*0.8=0.32$
If anything is unclear, ask please, or contemplate the next image, where we extend the branches such that the two in the middle connect, for a bit :)


## Binomial distribution.

The probability mass function is packed in R function named dbinom(), which has parameters "x", which is the same as k aboveor in the wikipedia example, and "size" which is written there instead of n.
```{r}
dbinom(x=1,size=2,prob=0.8)
```

In short if you preserve the order of parameters like this, you can write also
```{r}
dbinom(1,2,0.8)
```
This is quite clearly the probability of ending in the middle.


## So the likelihood of tossing 3 heads out of 4 with the fair coin is
```{r}
dbinom(x=1,size=2,prob=0.8)
binom.l1<-dbinom(3,4,0.5) #fair coin
binom.l2<-dbinom(3,4,0.8) #tric coin
```
```{r,echo=F}
binom.l1
binom.l2
```

## The likelihoods of tossing 3 heads out of 4
with this function is just 4 times the likelihoods of the product of heads and tails in given order that we calculated before. That is because there are exactly four ways how to arrange three heads in the row of four tosses, or, more obviously, four places, where we can place the lone "tails", or "lone eagle" in Czech.
```{r, eval=F}
4*calcul.l1
4*calcul.l2
```

```{r, echo=F}
4*calcul.l1
4*calcul.l2
```


## Unsurprisingly
Since we multiplied BOTH by the same number, we arrive at the same proportion of hypothesis probability.
```{r}
binom.p1<-binom.l1/(binom.l1+binom.l2)
binom.p2<-binom.l2/(binom.l1+binom.l2)
```
```{r,echo=F}
binom.p1
binom.p2
```

## Check against the previous attempts
```{r,eval=F}
calcul.p1
calcul.p2

simul.p1
simul.p2
```
```{r,echo=F}
calcul.p1
calcul.p2

simul.p1
simul.p2
```

## All ways converge.
What if we, after those four tosses toss one more time - this time it falls "tails"?

## Recounting
We can just input "3 out of 5" everywhere instead of "3 out of 4". In the last - most nice and compact form:
```{r}
onemore.l1<-dbinom(3,5,0.5)
onemore.l2<-dbinom(3,5,0.8)

onemore.p1<-onemore.l1/(onemore.l1+onemore.l2)
onemore.p2<-onemore.l2/(onemore.l1+onemore.l2)
```
```{r,echo=F}
onemore.p1
onemore.p2
```
See how the probability shifted sightly towards the fair coin, since the number of heads and tails is more balanced now?

## Updating
We alreeady know something after 4 tosses
```{r}
binom.p1
binom.p2
```

## And now we toss "tails".
That is 0 "heads" out of 1 toss.
```{r}
onetails.p1<-dbinom(0,1,0.5)
onetails.p2<-dbinom(0,1,0.8)
```
```{r,echo=F}
onetails.p1
onetails.p2
```
Which is obviously much more likely with the fair coin that with a coin that is expected to came out heads in 4 tosses out of five.

## Updating
The probability that we already have - let us call it "prior probability" as in "probability of hypothesis prior to the fifth toss".

Like adding $*0.5$ or $*0.2$ to our row of
$0.8*0.2*0.8*0.8$
and get
$0.8*0.2*0.8*0.8*0.2$ for the tric coin
and 
$0.5*0.5*0.5*0.5$
and get
$0.5*0.5*0.5*0.5*0.5$ for fair coin

## Updating works with likelihood
```{r}
updated.l1<-binom.l1*dbinom(0,1,0.5)
updated.l2<-binom.l2*dbinom(0,1,0.8)

updated.p1<-updated.l1/(updated.l1+updated.l2)
updated.p2<-updated.l2/(updated.l1+updated.l2)
```
```{r,echo=F}
updated.p1
updated.p2
```

## Updating works with probability
```{r}
updated.l1<-binom.p1*dbinom(0,1,0.5)
updated.l2<-binom.p2*dbinom(0,1,0.8)

updated.p1<-updated.l1/(updated.l1+updated.l2)
updated.p2<-updated.l2/(updated.l1+updated.l2)
```
```{r,echo=F}
updated.p1
updated.p2
```

## And it is the same as unupdated "from scratch" version
```{r}
onemore.p1
onemore.p2
```

## And there we are there
at gates of Bayes' theorem.

$$ P(H|d)=\frac{P(d|H)P(H)}{P(d)}$$


If we have two hypotheses like now, we can say that it is

$$ P(H_1|d)=\frac{P(d|H_1)P(H_1)}{P(d|H_1)P(H_1)+P(d|H_2)P(H_2)}$$
$$ P(H_2|d)=\frac{P(d|H_2)P(H_2)}{P(d|H_1)P(H_1)+P(d|H_2)P(H_2)}$$

## See? 
Numerators are different, denominators are the same.
Numerators are what makes the difference.
$$ P(H_1|d)=\frac{P(d|H_1)P(H_1)}{P(d|H_1)P(H_1)+P(d|H_2)P(H_2)}$$
$$ P(H_2|d)=\frac{P(d|H_2)P(H_2)}{P(d|H_1)P(H_1)+P(d|H_2)P(H_2)}$$
## Updating 
Integrates information from multiple domains.
(Your friend who exposes you to the cointosses might be a cheater an you know it.)

```{r}
l1<-dbinom(3,4,0.5)*0.1 #numerators
l2<-dbinom(3,4,0.8)*0.9
p1<-l1/(l1+l2) #divide them with a common denominator
p2<-l2/(l1+l2)
```
```{r,echo=F}
c(p1=p1,p2=p2)
```

## After the additional toss of "tails"
```{r}
l1<-dbinom(0,1,0.5)*p1
l2<-dbinom(0,1,0.8)*p2 
p1<-l1/(l1+l2)
p2<-l2/(l1+l2)
```
```{r,echo=F}
c(p1=p1,p2=p2)
```

## Any so called "posterior probability"
can be fed into the "Bayes' theorem" or rather the "hypothesis credibility machine" as "prior probability" before the new data are observed. And the new data then update the hypotheses probabilities in a nice formal unambiguous way. Bayesian statistics is the best formal epistemology that we have so far.

## The "prior" for short was always there

```{r}
l1<-dbinom(3,4,0.5)*0.5 #numerators
l2<-dbinom(3,4,0.8)*0.5

p1<-l1/(l1+l2) #divide by their sum
p2<-l2/(l1+l2)
```
```{r,echo=F}
c(p1=p1,p2=p2)
```

## Plus the additional "tails"
```{r}
l1<-dbinom(0,1,0.5)*p1
l2<-dbinom(0,1,0.8)*p2 
p1<-l1/(l1+l2)
p2<-l2/(l1+l2)
```
```{r,echo=F}
c(p1=p1,p2=p2)
```

## For the sake of absurdity:
by observation

```{r}
#First toss (heads) 
l1<-dbinom(1,1,0.5)*0.5
l2<-dbinom(1,1,0.8)*0.5
p1<-l1/(l1+l2)
p2<-l2/(l1+l2)
```
```{r,echo=F}
c(p1=p1,p2=p2)
```

## For the sake of absurdity:
by observation

```{r}
#Second toss (tails) 
l1<-dbinom(0,1,0.5)*p1
l2<-dbinom(0,1,0.8)*p2
p1<-l1/(l1+l2)
p2<-l2/(l1+l2)
```
```{r,echo=F}
c(p1=p1,p2=p2)
```

## For the sake of absurdity:
by observation

```{r}
#Third toss (heads) 
l1<-dbinom(1,1,0.5)*p1
l2<-dbinom(1,1,0.8)*p2
p1<-l1/(l1+l2)
p2<-l2/(l1+l2)
```
```{r,echo=F}
c(p1=p1,p2=p2)
```

## For the sake of absurdity:
by observation

```{r}
#Fourth toss (heads) 
l1<-dbinom(1,1,0.5)*p1
l2<-dbinom(1,1,0.8)*p2
p1<-l1/(l1+l2)
p2<-l2/(l1+l2)
```
```{r,echo=F}
c(p1=p1,p2=p2)
```

## For the sake of absurdity:
by observation

```{r}
#Fifth toss (tails) 
l1<-dbinom(0,1,0.5)*p1
l2<-dbinom(0,1,0.8)*p2
p1<-l1/(l1+l2)
p2<-l2/(l1+l2)
```
```{r,echo=F}
c(p1=p1,p2=p2)
```

## Fun useless fact:
Special case of binomial distribution where we toss only one coin and observe heads or tails is for some reason called "Bernoulli distribution"

## Task 0
Reformalize the system such that "tails" is TRUE instead of "heads" and demonstrate, how you can reach the same conclusion (choose any approach you have seen above).

## Epilog

This all was well known in 18th century (The Theorem was first presented by Richard Price to Royal society in 1783, two years after the death of Thomas Bayes).

This is the original and perfect statistics.

# VERY VERY STUPID SAMPLER

## The question is
what if we wish to model the function P(H) not as relative proportion of probability of two discrete alternative hypotheses

**What would you do?**

## Would you
just chop the continuum into regularly spaced discrete hypotheses right away? Would you be willing to say that first significant digit does not matter?
```{r}
(Hp<-seq(0,1,by=0.1))
```

## Or perhaps second digit?
```{r}
(Hp<-seq(0,1,by=0.01))
```

Ok, let us go with that.


## What is
the likelihood that such coins create 3 heads out of 4 (imagine they all have the same prior probability 1/number of hypotheses)?

```{r}
prior.p<-1/length(Hp) #prior probability of each hypothesis is slightly under 1 percent
prior<-rep(prior.p,length(Hp))
```
```{r, echo=F}
prior
```

## May be

```{r, echo=F}
plot(Hp,prior,type="l",xaxs="i",yaxs="i",ylim=c(0,1),col=2)
```

You are no longer in the realm of discrete hypothesis

## Continuum of hypotheses
the area under this line is $1*1$). So you do not really need this vector.

```{r}
prior<-rep(1,length(Hp))
```
```{r, echo=F}
plot(Hp,prior,type="l",xaxs="i",yaxs="i",ylim=c(0,1.1),col=2)
```

## likelihoods  
```{r}
(l<-dbinom(3,4,Hp)) 
```

## Good job
Notice that since there are both heads and tails, the likelihoods of the extreme hypotheses (p=0 and p=1) is perfect 0

## The numerators
products of data likelihood times the prior probabilities
```{r}
(numerators<-l*prior)
```

```{r}
sum(numerators)
```

## Numerators do not sum up to 1 (neither do likelihoods)
divide
```{r}
(post<-numerators/sum(numerators))
```

## There is your graphical representation of the continuum
```{r}
plot(Hp,post,type="l",lwd=2,col=4)
```

## We named it post for posterior
but remember that every posterior can become prior of the next round.
```{r}
post<-(dbinom(0,1,Hp)*post)/sum((dbinom(0,1,Hp)*post))
plot(Hp,post,type="l",lwd=2,col=4)
```

## The peak is at the observed proportion
since we started from the perfectly flat prior
```{r}
plot(Hp,post,type="l",lwd=2,col=4)
abline(v=3/5,col=2)
```

## Is this the end?
It might be for simple systems with low number of parameters. This technique is called Grid approximation. But imagine you had some system that - unlike coin - has more than one parameter.

# Some examples from the audience.

## My example

Dice that has six sides: 1,2,3,4,5,6 and there are all sorts of probabilites as you can imagine. Fair 
```{r}
rep(1/6,6)
```


## Unfair dice might be
```{r}
(6:1)/sum(6:1)
```

But there are many more.

## 1 vs the rest behaves like a coin, it can fall with probability
```{r}
(p1<-seq(0,1,by=0.01))
```

## The rest?
You just take whatever is left and divide it with the same sensitivity
```{r}
p2<-lapply(p1,function(x){seq(0,1-x,0.01)})
p2[1:10]

d<-data.frame(p1=rep(p1,sapply(p2,length)),p2=unlist(p2))
nrow(d)
```

## The same goes for p3
only I divide the remnant after summing p1 and p2
```{r}
p3<-lapply(rowSums(d),function(x){seq(0,1-x,0.01)})
d<-data.frame(p1=rep(d$p1,sapply(p3,length)),p2=rep(d$p2,sapply(p3,length)),p3=unlist(p3))
nrow(d)
```

## p4 (things get out of hand)
```{r}
p4<-lapply(rowSums(d),function(x){seq(0,1-x,0.01)})
d<-data.frame(p1=rep(d$p1,sapply(p4,length)),
              p2=rep(d$p2,sapply(p4,length)),
              p3=rep(d$p3,sapply(p4,length)),
              p4=unlist(p4))
nrow(d)
```

# Now I have over 4 million hypothesis and I am not done.

## I am at 4 parameters
The question for you is, how many parameters has a 6-sided dice.) and still the same precision (differences smaller than neglect).

## If I did not care
about having different precision for different sides and about duplicates (but I should care about those!), if my parameters were independent:

```{r}
Hypotheses<-expand.grid(p1=Hp,p2=Hp)
head(Hypotheses)
nrow(Hypotheses)
```

## basically
```{r}
length(Hp)*length(Hp) #this much
```

## If I had three such parameters
```{r}
Hypotheses<-expand.grid(p1=Hp,p2=Hp,p3=Hp)
nrow(Hypotheses)
```

## I would now need to calculate data likelihood for more than a million hypotheses!

```{r,eval=F}
length(Hp)^1 #one parameter between 0 and 1 with the same precision
length(Hp)^2 #two parameters
length(Hp)^3 #three parameters
length(Hp)^4 #four parameters
length(Hp)^5 #five parameters (well over 100 milliard hypotheses)
```

```{r,echo=F}
length(Hp)^c(1:5)
```

## Offer
(Who invents nice scalable example for this (the problem with dice is mutual dependence of likelihood of sides), such example that we use from the next year on, gets the credit for this course for free!)

## If you do not sample one coin
but you want to find out what is the distribution of coins (fair and unfair) in some population, so you visit $10$ pubs in each of $195$ countries and in each pub, you ask $10$ people to toss their favorite coin $10$ times

```{r}
length(Hp)^(10*10*10*195)
```
You are beyond the numebr of atoms in the observable universe.

## For this reason
people almost always use not regularly spaced, but randomly drawn hypotheses (which, in effect, frequently leads to very similar ends).


# Finally!

## Let us program the stupidest random sampler ever!
(The thing we did was not yet proper sampling, right?)

## Bayesian statistics is in essence close to random simulations

Instead of 101 equally spaced hypothesis, I draw 10000 random hypotheses and then, I draw a subset of 1000 these hypotheses, and the probability of second draw will be proportional to the "likelihood of the data if given hypothesis" $P(d|H)$

## 10000 random coins
function runif generates random numbers from a flat uniform distribution
```{r}
Hpr<-runif(10000,min=0,max=1)
l<-dbinom(3,4,Hpr) #likelihood that they produced the data
```

## 1000 random coins
from the original 10000
```{r}
samples<-sample(Hpr,1000,replace=T,prob=l)
str(samples)
```

## The samples
is just a long row of parameter values that correspond to plausible hypotheses. That is, what a sampled posterior is. 

## Histogram of samples
```{r}
hist(samples)
```
You can already see that it resembles the curve that we got from the analytical approach

#Density by KDE
I need to multiply the original "post" variable, because now the discrete probabilities sum up to 1, not the area of the whole polygon such as is the default.
```{r}
dens<-density(samples)
plot(Hp,post*100,type="l",lwd=2,col=4)
lines(dens$x,dens$y,lwd=2,col=3)
```
See? It is quite similar.

## Before we disect The Stupidest Sampler Ever
let me introduce an example that really needs more than 1 parameter. And let me (re)introduce logarithms along the way.

## Power of logarithm
Logarithm is defined as a function inverse to exponential function

```{r, echo=FALSE}
mycol<-"#006ABE"

p1<-0.3
p2<-1-p1

gridcol<-"#808080"
gridcol2<-"#80A0A0"

x<-seq(0.05,3,b=0.001)

par(mar=c(3,3,1.5,1),mgp=c(2,0.7,0))
plot(x,log(x),type="l",col=mycol,xlim=c(0,3))
abline(v=1,lty=2,col=gridcol)
abline(h=0,lty=2,col=gridcol)
```

## Power of logarithm
Logarithm is defined as a function inverse to exponential function
$$5=e^{\text{log}(5)}$$
$$5=2^{\text{log}(5,\text{base}=2)}$$

```{r}
log(exp(5))
exp(log(3))
```

## Power of logarithm
Invented to translate between multiplicative and additive processes
$$5=e^{\text{log}(5)}$$
$$5=2^{\text{log}(5,\text{base}=2)}$$


## Power of logarithm
Which is used in probability calculations on stupid discrete-states computers :)
But also in data visualizations and handling. It is important to understand logarithms if you want to practice anything science-like.

## Power of logarithm
Logarithms transform clumsy multiplicative processes to easily handled additive process.
Logarithms make big numbers (>1) smaller and small numbers (<1) negative.

When our ancestors needed to multiply 3 large numbers, They took their logarithms
```{r}
N<-c(225698,556877988,3546899)
(logN<-log(N))
```

## Power of logarithm
Added the logarithms together
```{r}
(sumN<-sum(logN))
```

And then raised e (natural logarithm base) to the resulting sum
```{r}
exp(sumN)
```

## Now we have computers
```{r}
prod(N)
```

But logarithms are still very useful

## Let me demonstrate:
First I will define a neater plotting function than the default one
```{r}
myplot<-function(x,y,y2=NULL,rounding=2,mycol="#2288FF",...){
  namx<-deparse(substitute(x))
  namy<-deparse(substitute(y))
  
  par(mar=c(4,4,4,1),mgp=c(2.2,1,0))
  plot(x,y,col=mycol,pch=16,xlab=namx,ylab=namy,...)
}
```

## Upload some data
```{r}
d<-read.table("gdppopdata.txt" ,sep="\t",header=T)
#There are some NAs for bigger units such as EU or OECD, that I discard
d<-d[!is.na(d$population),]
```

## See what we got here
On natural scales - GDP and population size, the data are messy, unbalanced, horribly heteroscedastic, possibly nonlinear 
```{r, echo=F}
myplot(d$population,d$totalGDP)
text(d$population,d$totalGDP,d$abr,pos=3,xpd=T)
```

## See what we got here
But that is because the process generating population size and GDP is not additive

```{r, echo=F}
myplot(d$population,d$totalGDP)
text(d$population,d$totalGDP,d$abr,pos=3,xpd=T)
```

## See it loged
```{r}
myplot(log(d$population),log(d$totalGDP))
text(log(d$population),log(d$totalGDP),d$abr,pos=3,xpd=T)
```


## You can change the base
```{r,eval=F}
?log
```
```{r,echo=F}
myplot(log(d$population,10),log(d$totalGDP,10))
```

## GDP is in millions, population in thousands
Czech republic is the red point
```{r, eval=F}
myplot(log(d$population*1000,10),log(d$totalGDP*1000000,10))
points(log(d$population*1000,10)[d$abr=="CZE"],log(d$totalGDP*1000000,10)[d$abr=="CZE"],col=2,pch=16)
abline(v=log(d$population*1000,10)[d$abr=="CZE"],col=2,lty=2)
abline(h=log(d$totalGDP*1000000,10)[d$abr=="CZE"],col=2,lty=2)
```


## GDP is in millions, population in thousands
Czech republic is the red point
```{r, echo=F}
myplot(log(d$population*1000,10),log(d$totalGDP*1000000,10))
points(log(d$population*1000,10)[d$abr=="CZE"],log(d$totalGDP*1000000,10)[d$abr=="CZE"],col=2,pch=16)
abline(v=log(d$population*1000,10)[d$abr=="CZE"],col=2,lty=2)
abline(h=log(d$totalGDP*1000000,10)[d$abr=="CZE"],col=2,lty=2)
```

## How about base 1.1
```{r}
better<-d
better$population<-log(d$population*1000,1.1)
better$totalGDP<-log(d$totalGDP*1000000,1.1)
```

## If we plot it and find Czech republic on the graph
```{r,echo=FALSE}
myplot(better$population,better$totalGDP)
points(better$population[better$abr=="CZE"],better$totalGDP[better$abr=="CZE"],col=2,pch=16)
abline(v=better$population[better$abr=="CZE"],col=2,lty=2)
abline(h=better$totalGDP[better$abr=="CZE"],col=2,lty=2)
```

## We can easily talk about differences between countries on this scale
```{r}
cze<-better$totalGDP[better$abr=="CZE"]
usa<-better$totalGDP[better$abr=="USA"]
```
```{r,echo=F}
c(cze=cze,usa=usa)
```

## difference between them
```{r}
(dif<-usa-cze)
```

The difference between Czechia and USA is 41. That means to get USA GDP, we would have to multiply our current GDP by 1.1 41 times (that means increase our GDP by 10% than increase the result by 10% again and so on and so on...)

```{r,eval=F}
1.1*1.1
1.1*1.1*1.1
1.1^3
1.1^dif
```
```{r,echo=F}
1.1^dif
```
The difference between Czech and USA GDP is 51.29 which means that USA has 51times higher GDP than Czech republic. All it takes is 41 increases by 10%

## We can demonstrate that the numbers fit
```{r,eval=F}
multip<-1.1^dif

d$totalGDP[d$abr=="CZE"]*multip
d$totalGDP[d$abr=="USA"]
```
```{r,echo=F}
multip<-1.1^dif

d$totalGDP[d$abr=="CZE"]*multip
d$totalGDP[d$abr=="USA"]
```

## What to do?
Such large numbers are, again, not very good for our imagination. Can we easily try out conversions between different bases?

## Sure!
```{r}
log(1.1^dif,base=10)
```
Difference that is 41 on a scale of base 1.1 is not even two on the scale of base 10
```{r}
log(1.1^dif,base=2)
```

## So Let us go with that!
```{r}
best<-d
best$population<-log(d$population*1000,2)
best$totalGDP<-log(d$totalGDP*1000000,2)
```

## The plot
```{r,echo=F}
myplot(best$population,best$totalGDP)
points(best$population[best$abr=="CZE"],best$totalGDP[best$abr=="CZE"],col=2,pch=16)
abline(v=best$population[best$abr=="CZE"],col=2,lty=2)
abline(h=best$totalGDP[best$abr=="CZE"],col=2,lty=2)
```

## Set a reference to 0
and report the units of multiplication towards this threshold.

```{r}
best$population<-best$population-best$population[best$abr=="CZE"]
best$totalGDP<-best$totalGDP-best$totalGDP[best$abr=="CZE"]
```

## Now Czech republic is at 0
and all other scores represent *2 (positive) or /2 (negative) changes in GDP or population size 
```{r,echo=F}
myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
abline(v=0,col=2,lty=2)
abline(h=0,col=2,lty=2)
```

## So how to parametrize such a system into a set of plausible hypotheses?

Let us pretend now, that we did  never see the data (that would be cheating), but we try to model some non-deterministic linear relationship. Obviously one of possible linear relationships is a a straight line - i.e. no relationship, independence of GDP on population.

## So how to parametrize such a system into a set of plausible hypotheses?
A thing that high school maths desperately lacks, in my opinion, is the introduction of non-deterministic equivalent of this symbol $=$

## So how to parametrize such a system into a set of plausible hypotheses?
It is called tilde and it looks like this $\sim$
We do not read it "is equal to" but perhaps "is randomly drawn from a distribution"

## There is an empty plot that calls for data
```{r}
myplot(best$population,best$totalGDP,type="n")
```

## How might such data arise?
```{r}
myplot(best$population,best$totalGDP,type="n")
points(0,0,col=2,pch=16)
```
We know that Czechia is at point 0,0

## But should that country be there?

How much more value of the final goods and services should Czechia produce given the almost 11 million people that live here.

## But should that country be there?
We know that the number we have in our data is 0. But that is the data! that is not the predictive system that is not deterministic. We can say, that this 0 is drawn from a distribution around expected value $a$.

## Let us assume
that this $a$ (how much Czech GDP should be) is 1.
```{r}
myplot(best$population,best$totalGDP,type="n");points(0,1,col=3,pch=16)
```
1 on base-2 logarithmic scale means that Czech GDP should be twice as big than it is.

## We, of course, need to define a distribution.
The distribution will not be binomial as distribution of coin tosses, obviously. It will be a distribution that allows to draw any number, possibly positive as well as negative, around an expected mean $a$, without a preference for higher or lower numbers.

## Choosing a distribution can seem like a challenge at a first sight
but really, it is not. We have a maximum entropy criteria. See here: <https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution>
There is a super useful table!

# If you use maximum entropy distribution, you do not assume anything you do not actually want to assume.

## (See chapter 10 in Rethinking)
```{r}
curve(exp(-x^2),xlim=c(-3,3),col=4)
```
All you need to know about its shape is, that is basically $e$ to the power of $-x^2$. <https://en.wikipedia.org/wiki/Normal_distribution>

#So in such a  system:
GDP of czechia is drawn from a nromal distribution with mean $a$ (for example $a=1$) and standard deviation $sigma$ (for example $sigma=0.5$)

```{r}
a<-1
sigma<-0.5
```

#So in such a  system:
GDP of czechia is drawn from a nromal distribution with mean $a$ (for example $a=1$) and standard deviation $sigma$ (for example $sigma=0.5$)
```{r}
myplot(best$population,best$totalGDP,type="n")
points(0,a,col=3,pch=16)
```

## Let us generate
50 possible Czechias from this distribution and put them into the plot.
```{r,eval=F}
(newczech<-rnorm(50,mean=a,sd=sigma))
myplot(best$population,best$totalGDP,type="n")
points(0,a,col=3,pch=16)
points(rep(0,50),newczech,col="#88888820",pch=16)
points(0,0,col=2,pch=16)
```

## Let us generate
50 possible Czechias from this distribution and put them into the plot.
```{r,echo=F}
newczech<-rnorm(50,mean=a,sd=sigma)

myplot(best$population,best$totalGDP,type="n")
points(0,a,col=3,pch=16)
points(rep(0,50),newczech,col="#88888820",pch=16)
points(0,0,col=2,pch=16)
```

## Let us calculate
The red dot - the GDP=0 in the data is at the very edge of the generated values, but if we check the value of probability density function, we see, that it is indeed quite possible to draw it from such a distribution.
```{r}
dnorm(0,mean=a,sd=sigma)
```

## Let us calculate
It is obviously much more likely to draw numbers close to the mean.
```{r}
dnorm(1,mean=a,sd=sigma)
```

## Add one more parameter.
We already have the expected value for Czechia, now we only need a GDP change with doubling the population.

$b=-1$ it is.
```{r}
b<- -1
```

## So now there are expected value per log population size
```{r,eval=F}
myplot(best$population,best$totalGDP,type="n")
points(0,a,col=3,pch=16)
points(rep(0,50),newczech,col="#88888820",pch=16)
points(0,0,col=2,pch=16)
abline(a=a,b=b,col=3)
```

## So now there are expected value per log population size
```{r,echo=F}
myplot(best$population,best$totalGDP,type="n")
points(0,a,col=3,pch=16)
points(rep(0,50),newczech,col="#88888820",pch=16)
points(0,0,col=2,pch=16)
abline(a=a,b=b,col=3)
```

## So now there are expected value per log population size
And around it I can generate some random data: 100 new countries
```{r}
n<-100
rpop<-runif(n,-4,6)
```
We have their random populations uniformly distributed between -4 and 6

## We calculate the expected GDP for each country
with the simple linear equation
And generate random deviation around each expected number
```{r}
mu<-a+b*rpop
rGDP<-rnorm(n,mu,sigma)
```

## Czechia blends quite well in
```{r, eval=F}
myplot(best$population,best$totalGDP,type="n")
points(0,a,col=3,pch=16)
points(rep(0,50),newczech,col="#88888820",pch=16)
points(0,0,col=2,pch=16)
points(rpop,rGDP,col="#888888",pch=16)
```

## Czechia blends quite well in
```{r, echo=F}
myplot(best$population,best$totalGDP,type="n")
points(0,a,col=3,pch=16)
points(rep(0,50),newczech,col="#88888820",pch=16)
points(0,0,col=2,pch=16)
points(rpop,rGDP,col="#888888",pch=16)
```

## So this is one of my possible hypotheses:
$a=1$, $b=-1$, $\sigma=0.5$, my system has three parameters, so each hypothesis that i formulate within the system has these three parameters.

## It is obvious that it is not a very good system.
```{r,echo=F}
myplot(best$population,best$totalGDP,type="n")
points(0,a,col=3,pch=16)
points(rep(0,50),newczech,col="#88888820",pch=16)
points(0,0,col=2,pch=16)
points(rpop,rGDP,col="#888888",pch=16)
points(best$population,best$totalGDP,col=4,pch=16)
```

# Let us use our **STUPIDEST SAMPLER EVER** to get the distribution of likely triplets of parameter values

## When I generate random hypotheses
I need to specify random distribution from which I am even willing to draw the hypotheses (similarly as I did above with the single coin parameter p).

I will use maximum entropy criteria to pick these distributions - my initial "priors".

I use relatively wide and unbiased initial priors.

## Random hypothesis generation
This is how many random hypotheses I generate
```{r}
nH<-100000
a<-rnorm(nH,mean=0,sd=1) #prior for a
b<-rnorm(nH,mean=0,sd=1) #prior for b
sigma<-rexp(nH,rate=1)   #exponential prior for sigma

hypotheses<-data.frame(a,b,sigma)
```

## Calculate data likelihood
for the first hypothesis
```{r}
i<-1
h<-hypotheses[i,]
per.point<-dnorm(best$totalGDP,h$a+h$b*best$population,h$sigma) #likelihood per point
prod(per.point)
```

## The data likelihood
for this hypothesis is extremely low
```{r}
h
```

## Maybe i will be more lucky elsewhere.
I will turn the likelihood into a function:

```{r}
mylikeli<-function(i){
  h<-hypotheses[i,]
  per.point<-dnorm(best$totalGDP,h$a+h$b*best$population,h$sigma) #likelihood per point
  return(prod(per.point))
}
```

## Calculate
$$P(d|H)$$ for each random hypothesis
```{r}
hypotheses$data.l<-sapply(1:nrow(hypotheses),mylikeli)
```

## There are some
that allow for the generation of data as they are
```{r}
str(sort(hypotheses$data.l,decreasing=T))
```

## Second raffle
So let us draw again some of them, the probability that a hypothesis is drawn is proportional to the data likelihood
```{r}
sampled.h<-sample(1:nH,1000,replace=T,prob=hypotheses$data.l)
samples<-hypotheses[sampled.h,]
head(samples)
```

## I have a set of plausible hypotheses.
This is the first of them
```{r, eval=F}
myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
abline(a=samples[1,]$a,b=samples[1,]$b)
abline(a=samples[1,]$a+samples[1,]$sigma*2,b=samples[1,]$b,lty=2)
abline(a=samples[1,]$a-samples[1,]$sigma*2,b=samples[1,]$b,lty=2)
```

## I have a set of plausible hypotheses.
This is the first of them
```{r, echo=F}
myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
abline(a=samples[1,]$a,b=samples[1,]$b)
abline(a=samples[1,]$a+samples[1,]$sigma*2,b=samples[1,]$b,lty=2)
abline(a=samples[1,]$a-samples[1,]$sigma*2,b=samples[1,]$b,lty=2)
```

## 2 SDs on each side?
(You can calculate the density between n sigmas with cumulative distribution function)
```{r}
pnorm(2)-pnorm(-2)
```

## This is second hypothesis
```{r, eval=F}
myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
abline(a=samples[2,]$a,b=samples[2,]$b)
abline(a=samples[2,]$a+samples[2,]$sigma*2,b=samples[2,]$b,lty=2)
abline(a=samples[2,]$a-samples[2,]$sigma*2,b=samples[2,]$b,lty=2)
```

## This is second hypothesis
```{r, echo=F}
myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
abline(a=samples[2,]$a,b=samples[2,]$b)
abline(a=samples[2,]$a+samples[2,]$sigma*2,b=samples[2,]$b,lty=2)
abline(a=samples[2,]$a-samples[2,]$sigma*2,b=samples[2,]$b,lty=2)
```

## 50 likely hypotheses
(but without the sigma parameter for clarity)
```{r, eval=F}
myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
for(i in 1:50){
  abline(a=samples[i,]$a,b=samples[i,]$b,col="#80808080")
}
```

## 50 likely hypotheses
(but without the sigma parameter for clarity)
```{r, echo=F}
myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
for(i in 1:50){
  abline(a=samples[i,]$a,b=samples[i,]$b,col="#80808080")
}
```

## Hypotheses more or less agree on some corridor.
Better visual than a bunch of rods
```{r}
x<-seq(-7,7,0.01)
y<-sapply(1:nrow(samples),function(i){samples[i,]$a+samples[i,]$b*x})
str(y)
muy<-apply(y,1,mean)
CIy<-apply(y,1,PI,prob=0.95)
```
This CI stands for compatibility interval

## Hypotheses more or less agree on some corridor.
Better visual than a bunch of rods
```{r,eval=F}
myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
lines(x,muy)
shade(CIy,x)
```

## Hypotheses more or less agree on some corridor.
Better visual than a bunch of rods
```{r,echo=F}
myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
lines(x,muy)
shade(CIy,x)
```

## There is an alternative to PI
**HPDI: Highest Posterior Density Interval**
```{r,eval=F}
HPDIy<-apply(y,1,HPDI,prob=0.95)
myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
lines(x,muy)
shade(HPDIy,x)
```

## There is an alternative to PI: HPDI
```{r,echo=F}
HPDIy<-apply(y,1,HPDI,prob=0.95)
myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
lines(x,muy)
shade(HPDIy,x)
```

very frequently these two methods give almost identical results.


## Summarize the hypotheses by parameter
(you might find this under the name of "marginal projections of joint posterior distribution" more abut this to come).

```{r}
summary(samples$a)
PI(samples$a,prob=0.95)
```
With probability 95% a (difference between expected GDP of Czechia and the realized total GDPis close to 0).

If we use the PI function (first function from the rethinking package that we use), it does not give 95% but 89% CI by default.

## Parameter b

```{r}
mu.b<-summary(samples$b)
CI.b<-PI(samples$b,prob=0.95)
```
```{r,echo=F}
mu.b
CI.b
```

## Per doubling the population
(move one step right along the x axis), the GDP is expected to grow by factor of approx
```{r}
2^mu.b["Mean"]
```

95% CI of this estimate is
```{r}
2^CI.b
```

## GDP grows with population
but it is unlikely to be twice as big for a population that is twice as big.

(Again, to avoid so called "Galton's fallacy", remember that this doe not prove any causation, because we do not have GDP or population in time series).

## We can summarize parameters nicely graphicaly
```{r,eval=F}
plot(NULL,xlim=c(-2,2),ylim=c(3.5,0.5),xlab="parameter value",ylab="",yaxt="n")
axis(2,at=c(1,2,3),labels=c("a","b","sigma"),las=2)
abline(h=1:3)
toplot<-samples$a
dens<-density(toplot)
sc<-0.1 #You need to define a scaler not to draw to big density plots
polygon(dens$x,1-dens$y*sc,col=4)
lines(PI(toplot,prob=0.95),c(1,1),lwd=3)
points(mean(toplot),1,lwd=2,cex=1.2,bg=0,pch=21)
```

## We can summarize parameters nicely graphicaly
```{r,echo=F}
plot(NULL,xlim=c(-2,2),ylim=c(3.5,0.5),xlab="parameter value",ylab="",yaxt="n")
axis(2,at=c(1,2,3),labels=c("a","b","sigma"),las=2)
abline(h=1:3)
toplot<-samples$a
dens<-density(toplot)
sc<-0.1 #You need to define a scaler not to draw to big density plots
polygon(dens$x,1-dens$y*sc,col=4)
lines(PI(toplot,prob=0.95),c(1,1),lwd=3)
points(mean(toplot),1,lwd=2,cex=1.2,bg=0,pch=21)
```

## I can pack this to a nice function
```{r}
drawDist<-function(toplot,y,sc=0.1){
  dens<-density(toplot)
  polygon(dens$x,y-dens$y*sc,col=4)
  lines(PI(toplot,prob=0.95),c(y,y),lwd=3)
  points(mean(toplot),y,lwd=2,cex=1.2,bg=0,pch=21)
}
```

## Plot with all parameters
```{r, eval=F}
plot(NULL,xlim=c(-2,2),ylim=c(3.5,0.5),xlab="parameter value",ylab="",yaxt="n")
axis(2,at=c(1,2,3),labels=c("a","b","sigma"),las=2)
abline(h=1:3)
drawDist(samples$a,y=1)
drawDist(samples$b,y=2)
drawDist(samples$sigma,y=3)
abline(v=seq(-2,2,1),lty=2)
```

## Plot with all parameters
```{r, echo=F}
plot(NULL,xlim=c(-2,2),ylim=c(3.5,0.5),xlab="parameter value",ylab="",yaxt="n")
axis(2,at=c(1,2,3),labels=c("a","b","sigma"),las=2)
abline(h=1:3)
drawDist(samples$a,y=1)
drawDist(samples$b,y=2)
drawDist(samples$sigma,y=3)
abline(v=seq(-2,2,1),lty=2)
```

## The last thing that we can plot
is the correlation between parameter values across samples. This is something that you usually put only in the supplement, but you should nevertheless check it out.

## There are many ways how to do it
but we can go with simple pairs function

```{r}
pairs(samples[,1:3])
```

It does not look like a complicated intercorrelated posterior.

## Why are samplers so useful?
There was no need to divide anything by a large sum. The big divisor in the denominator is the same for all hypotheses.

All sampling techniques that we will introduce use this awesome fact and they just output a random subset of a continuum of plausible hypothesis.

## Why are samplers so useful?
Our sampler explicitly divides the raffle into two steps to bring closer the idea of hypothesis $\text{prior} * \text{data likelihood}$ product. (Remember the forking paths from way above.) It is obviously much more effective to do it all in a single step. Stupidest sampler still generates a lot of junk per one random hypothesis that ends up in the final set of samples. Most samplers are more effective.

## Best sampler ever?
```{r}
#We need to prepare a data for it in nicer form
clean.d<-list(pop=best$population,
              gdp=best$totalGDP)

model<-ulam(alist(
  gdp ~ dnorm(a+b*pop,sigma),
  a ~ dnorm(0,1),
  b ~ dnorm(0,1),
  sigma ~ dexp(1)
),data=clean.d,chains=4,cores=4)
```

## Quick summary
```{r}
precis(model) #It is almost identical to the stupidest sampler
```

## Extracting samples is easy
```{r}
post<-extract.samples(model)
```


## TASK 1
**Visualize the parameter values from the posterior (object post).**
If you struggle, you can find the solution in TASK_SOLUTIONS.R line 1 

## TASK 2
**Draw the prediction line and the 99% Percentile Compatibility interval around it**
If you struggle, you can find the solution in TASK_SOLUTIONS.R line 11 

# Solution

## Plot parameter estimates
```{r,eval=F}
plot(NULL,xlim=c(-2,2),ylim=c(3.5,0.5),xlab="parameter value",ylab="",yaxt="n")
axis(2,at=c(1,2,3),labels=c("a","b","sigma"),las=2)
abline(h=1:3)
drawDist(post$a,y=1)
drawDist(post$b,y=2)
drawDist(post$sigma,y=3)
abline(v=seq(-2,2,1),lty=2)
```

## Plot parameter estimates
```{r, echo=F}
plot(NULL,xlim=c(-2,2),ylim=c(3.5,0.5),xlab="parameter value",ylab="",yaxt="n")
axis(2,at=c(1,2,3),labels=c("a","b","sigma"),las=2)
abline(h=1:3)
drawDist(post$a,y=1)
drawDist(post$b,y=2)
drawDist(post$sigma,y=3)
abline(v=seq(-2,2,1),lty=2)
```

## Plot regression line and CI
Notice the prob=0.99
```{r,eval=F}
x<-seq(-7,8,0.01)
y<-sapply(1:length(post$a),function(i){post$a[i]+post$b[i]*x})
muy<-apply(y,1,mean)
CIy<-apply(y,1,PI,prob=0.99)

myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
lines(x,muy)
shade(CIy,x)
```

## Plot regression line and CI
```{r,echo=F}
x<-seq(-7,8,0.01)
y<-sapply(1:length(post$a),function(i){post$a[i]+post$b[i]*x})
muy<-apply(y,1,mean)
CIy<-apply(y,1,PI,prob=0.99)

myplot(best$population,best$totalGDP)
points(0,0,col=2,pch=16)
lines(x,muy)
shade(CIy,x)
```

## Plot on the original scale
This steps add to each number the $log2$ of Czech population and then delogarithmize it by allying the inverse function $2^x$ on the logarithms. The division by thousand is here to cope with the fact that the original data are in thousands

## Plot on the original scale
```{r}
addpop<-log(d$population[best$abr=="CZE"]*1000,base=2)
head(cbind(
2^(best$population+addpop)/1000,
d$population)) #Check that it is the same
```

## Plot on the original scale
```{r,eval=F}
myplot(d$population,d$totalGDP)

addgdp<-log(d$totalGDP[best$abr=="CZE"]*1000000,base=2)
orig.scale.x<-2^(x+addpop)/1000
orig.scale.y<-2^(y+addgdp)/1000000

muy<-apply(orig.scale.y,1,mean)
CIy<-apply(orig.scale.y,1,PI,prob=0.95)
lines(orig.scale.x,muy)
shade(CIy,orig.scale.x)
```

## Plot on the original scale
```{r,echo=F}
myplot(d$population,d$totalGDP)

addgdp<-log(d$totalGDP[best$abr=="CZE"]*1000000,base=2)
orig.scale.x<-2^(x+addpop)/1000
orig.scale.y<-2^(y+addgdp)/1000000

muy<-apply(orig.scale.y,1,mean)
CIy<-apply(orig.scale.y,1,PI,prob=0.95)
lines(orig.scale.x,muy)
shade(CIy,orig.scale.x)
```

## Why did we bother
to show it all on a system that had to be logarithmized and delogarithmized back a forth? Because logarithms are super important in probability calculations.

Computers are stupid, look:
```{r}
#We have two vectors of small probabilities
A<-rep(c(0.001,0.0005,0.0003),100)
B<-rep(c(0.001,0.0005,0.0003),50)
```
```{r,echo=F}
str(list(
  A=A,B=B))
```

## Taking their products
returns simply 0
```{r,eval=F}
prod(A)
prod(B)
```
```{r,echo=F}
c(prod(A),prod(B))
```

## But if we log them
```{r}
logA<-log(A,10)
logB<-log(B,10)
```

## And sum the logs
```{r}
sumA<-sum(log(A,10))
sumB<-sum(log(B,10))
```
```{r,echo=F}
c(sumA,sumB)
```
We can clearly see, that the resulting probability of B product is much higher than resulting probability of A.

## Power of logarithm
Logarithm scales the positive part of the real number axis such that all numbers between 0 and 1 (most numbers in probability calculations) have the all negative part of the real axis for themselves, which is really useful if we represent continuous numbers on a discrete machine like our computer.

